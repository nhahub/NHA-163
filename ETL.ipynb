{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9717935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote test table to PostgreSQL!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgresConnectionTest\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/postgresql-42.7.3.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres_general:5432/postgres\"\n",
    "jdbc_props = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [(1, \"Spark test connection\"), (2, \"Postgres test passed!\")],\n",
    "    [\"id\", \"message\"]\n",
    ")\n",
    "\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"spark_test_table\") \\\n",
    "    .options(**jdbc_props) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"Successfully wrote test table to PostgreSQL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "099d74bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+------+----------+--------+--------+--------------+--------------+--------------+\n",
      "|student_id|family_id|student_name|gender|birth_date|grade   |city    |email         |phone         |full_name     |\n",
      "+----------+---------+------------+------+----------+--------+--------+--------------+--------------+--------------+\n",
      "|1         |219      |Sara        |Male  |2015-06-06|Grade 7 |Tanta   |aya1@mail.com |017.82878e+007|Sara Khaled   |\n",
      "|2         |244      |Heba        |Male  |2025-08-26|Grade 7 |Mansoura|ahm2@mail.com |018.16901e+007|Heba Nasser   |\n",
      "|3         |312      |Hussein     |Female|2012-07-23|Grade 8 |Tanta   |nou3@mail.com |NULL          |Hussein Omar  |\n",
      "|4         |4        |Mahmoud     |Female|2014-11-13|Grade 11|NULL    |ali4@mail.com |017.88741e+007|Mahmoud Nasser|\n",
      "|5         |115      |NULL        |Male  |2022-12-11|Grade 9 |Tanta   |NULL          |019.33359e+007|NULL          |\n",
      "|6         |75       |Omar        |Female|2018-04-25|Grade 7 |Tanta   |ade6@mail.com |014.13073e+007|Omar Hassan   |\n",
      "|7         |303      |Zeinab      |Male  |2018-06-01|Grade 11|Cairo   |moh7@mail.com |NULL          |Zeinab Tarek  |\n",
      "|8         |169      |Ahmed       |Male  |2023-02-04|Grade 8 |Tanta   |fat8@mail.com |015.97167e+007|Ahmed Nasser  |\n",
      "|9         |27       |Hassan      |Male  |2022-11-04|Grade 7 |Cairo   |hus9@mail.com |016.96466e+007|Hassan Saeed  |\n",
      "|10        |139      |Yara        |Male  |2022-06-21|Grade 8 |Cairo   |kar10@mail.com|015.66635e+007|Yara Nasser   |\n",
      "+----------+---------+------------+------+----------+--------+--------+--------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQLServerToPostgresDocker\") \\\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/opt/spark/jars/mssql-jdbc-13.2.1.jre8.jar,/opt/spark/jars/postgresql-42.7.3.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlserver_url = \"jdbc:sqlserver://192.168.1.7:1433;databaseName=EducationSystem_Raw;encrypt=false;trustServerCertificate=true;\"\n",
    "\n",
    "sqlserver_props = {\n",
    "    \"user\": \"sa\",\n",
    "    \"password\": \"sa12345\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT TOP 10 * FROM dbo.Students) AS t\",  # Ù‚Ø±Ø§Ø¡Ø© Ø¹ÙŠÙ†Ø© ØµØºÙŠØ±Ø© Ù„Ù„ØªØ£ÙƒØ¯\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afc85f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n",
      "+----------+---------+------------+------+----------+--------+----------+--------------+--------------+---------------+\n",
      "|student_id|family_id|student_name|gender|birth_date|grade   |city      |email         |phone         |full_name      |\n",
      "+----------+---------+------------+------+----------+--------+----------+--------------+--------------+---------------+\n",
      "|1         |219      |Sara        |Male  |2015-06-06|Grade 7 |Tanta     |aya1@mail.com |017.82878e+007|Sara Khaled    |\n",
      "|2         |244      |Heba        |Male  |2025-08-26|Grade 7 |Mansoura  |ahm2@mail.com |018.16901e+007|Heba Nasser    |\n",
      "|3         |312      |Hussein     |Female|2012-07-23|Grade 8 |Tanta     |nou3@mail.com |NULL          |Hussein Omar   |\n",
      "|4         |4        |Mahmoud     |Female|2014-11-13|Grade 11|NULL      |ali4@mail.com |017.88741e+007|Mahmoud Nasser |\n",
      "|5         |115      |NULL        |Male  |2022-12-11|Grade 9 |Tanta     |NULL          |019.33359e+007|NULL           |\n",
      "|6         |75       |Omar        |Female|2018-04-25|Grade 7 |Tanta     |ade6@mail.com |014.13073e+007|Omar Hassan    |\n",
      "|7         |303      |Zeinab      |Male  |2018-06-01|Grade 11|Cairo     |moh7@mail.com |NULL          |Zeinab Tarek   |\n",
      "|8         |169      |Ahmed       |Male  |2023-02-04|Grade 8 |Tanta     |fat8@mail.com |015.97167e+007|Ahmed Nasser   |\n",
      "|9         |27       |Hassan      |Male  |2022-11-04|Grade 7 |Cairo     |hus9@mail.com |016.96466e+007|Hassan Saeed   |\n",
      "|10        |139      |Yara        |Male  |2022-06-21|Grade 8 |Cairo     |kar10@mail.com|015.66635e+007|Yara Nasser    |\n",
      "|11        |230      |Farah       |Female|2011-07-14|Grade 9 |Cairo     |NULL          |013.84492e+007|Farah Ali      |\n",
      "|12        |86       |Rana        |Male  |2017-09-30|Grade 7 |Mansoura  |tar12@mail.com|019.99396e+007|Rana Ibrahim   |\n",
      "|13        |185      |Karim       |Male  |2021-10-31|Grade 8 |Cairo     |tar13@mail.com|NULL          |Karim Nasser   |\n",
      "|14        |303      |Adel        |Male  |2010-07-04|Grade 7 |Alexandria|NULL          |016.26218e+007|Adel Ibrahim   |\n",
      "|15        |185      |Karim       |Male  |2021-10-31|Grade 8 |Cairo     |tar13@mail.com|NULL          |Karim Mostafa  |\n",
      "|16        |221      |Fatma       |Male  |2011-12-15|Grade 7 |Cairo     |NULL          |011.4799e+007 |Fatma Omar     |\n",
      "|17        |217      |Mohamed     |Male  |2016-07-12|Grade 8 |Cairo     |hus16@mail.com|014.04083e+007|Mohamed Mahmoud|\n",
      "|18        |NULL     |Hussein     |Male  |2024-01-24|Grade 10|Giza      |yar17@mail.com|015.60571e+007|Hussein Hassan |\n",
      "|19        |301      |Salma       |Female|2015-10-18|NULL    |Cairo     |oma18@mail.com|012.9041e+007 |Salma Khaled   |\n",
      "|20        |275      |Hassan      |Female|2014-05-01|Grade 11|Cairo     |NULL          |013.41984e+007|Hassan Mostafa |\n",
      "+----------+---------+------------+------+----------+--------+----------+--------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_students = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT * FROM dbo.Students) AS t\",\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\")\n",
    "df_students.show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f02bd25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+------+----------+--------+----------+-----------------+--------------+---------------+\n",
      "|student_id|family_id|student_name|gender|birth_date|grade   |city      |email            |phone         |full_name      |\n",
      "+----------+---------+------------+------+----------+--------+----------+-----------------+--------------+---------------+\n",
      "|1         |219      |Sara        |Male  |2015-06-06|Grade 7 |Tanta     |aya1@mail.com    |017.82878e+007|Sara Khaled    |\n",
      "|2         |244      |Heba        |Male  |2025-08-26|Grade 7 |Mansoura  |ahm2@mail.com    |018.16901e+007|Heba Nasser    |\n",
      "|3         |312      |Hussein     |Female|2012-07-23|Grade 8 |Tanta     |nou3@mail.com    |01000000000   |Hussein Omar   |\n",
      "|4         |4        |Mahmoud     |Female|2014-11-13|Grade 11|Cairo     |ali4@mail.com    |017.88741e+007|Mahmoud Nasser |\n",
      "|5         |115      |Unknown     |Male  |2022-12-11|Grade 9 |Tanta     |unknown@gmail.com|019.33359e+007|NULL           |\n",
      "|6         |75       |Omar        |Female|2018-04-25|Grade 7 |Tanta     |ade6@mail.com    |014.13073e+007|Omar Hassan    |\n",
      "|7         |303      |Zeinab      |Male  |2018-06-01|Grade 11|Cairo     |moh7@mail.com    |01000000000   |Zeinab Tarek   |\n",
      "|8         |169      |Ahmed       |Male  |2023-02-04|Grade 8 |Tanta     |fat8@mail.com    |015.97167e+007|Ahmed Nasser   |\n",
      "|9         |27       |Hassan      |Male  |2022-11-04|Grade 7 |Cairo     |hus9@mail.com    |016.96466e+007|Hassan Saeed   |\n",
      "|10        |139      |Yara        |Male  |2022-06-21|Grade 8 |Cairo     |kar10@mail.com   |015.66635e+007|Yara Nasser    |\n",
      "|11        |230      |Farah       |Female|2011-07-14|Grade 9 |Cairo     |unknown@gmail.com|013.84492e+007|Farah Ali      |\n",
      "|12        |86       |Rana        |Male  |2017-09-30|Grade 7 |Mansoura  |tar12@mail.com   |019.99396e+007|Rana Ibrahim   |\n",
      "|13        |185      |Karim       |Male  |2021-10-31|Grade 8 |Cairo     |tar13@mail.com   |01000000000   |Karim Nasser   |\n",
      "|14        |303      |Adel        |Male  |2010-07-04|Grade 7 |Alexandria|unknown@gmail.com|016.26218e+007|Adel Ibrahim   |\n",
      "|15        |185      |Karim       |Male  |2021-10-31|Grade 8 |Cairo     |tar13@mail.com   |01000000000   |Karim Mostafa  |\n",
      "|16        |221      |Fatma       |Male  |2011-12-15|Grade 7 |Cairo     |unknown@gmail.com|011.4799e+007 |Fatma Omar     |\n",
      "|17        |217      |Mohamed     |Male  |2016-07-12|Grade 8 |Cairo     |hus16@mail.com   |014.04083e+007|Mohamed Mahmoud|\n",
      "|18        |NULL     |Hussein     |Male  |2024-01-24|Grade 10|Giza      |yar17@mail.com   |015.60571e+007|Hussein Hassan |\n",
      "|19        |301      |Salma       |Female|2015-10-18|Grade 10|Cairo     |oma18@mail.com   |012.9041e+007 |Salma Khaled   |\n",
      "|20        |275      |Hassan      |Female|2014-05-01|Grade 11|Cairo     |unknown@gmail.com|013.41984e+007|Hassan Mostafa |\n",
      "+----------+---------+------------+------+----------+--------+----------+-----------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_clean = df_students.fillna({\n",
    "    \"student_name\": \"Unknown\",\n",
    "    \"email\": \"unknown@gmail.com\",\n",
    "    \"phone\": \"01000000000\",\n",
    "    \"city\": \"Cairo\",\n",
    "    \"grade\": \"Grade 10\"\n",
    "})\n",
    "students_clean.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7424054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------------+---------+\n",
      "|student_id|student_name|email            |family_id|\n",
      "+----------+------------+-----------------+---------+\n",
      "|172       |Hussein     |mos164@mail.com  |231      |\n",
      "|762       |Reem        |sar740@mail.com  |70       |\n",
      "|954       |Nader       |oma926@mail.com  |33       |\n",
      "|1034      |Unknown     |unknown@gmail.com|3        |\n",
      "|1051      |Ali         |you1021@mail.com |152      |\n",
      "|1249      |Aya         |unknown@gmail.com|71       |\n",
      "|1465      |Zeinab      |lai1427@mail.com |321      |\n",
      "|8         |Ahmed       |fat8@mail.com    |169      |\n",
      "|521       |Hussein     |ali506@mail.com  |255      |\n",
      "|679       |Nader       |tar658@mail.com  |69       |\n",
      "|1107      |Nada        |unknown@gmail.com|122      |\n",
      "|1443      |Ahmed       |kar1405@mail.com |195      |\n",
      "|1469      |Unknown     |ree1431@mail.com |207      |\n",
      "|116       |Fatma       |ree111@mail.com  |297      |\n",
      "|157       |Amr         |unknown@gmail.com|160      |\n",
      "|317       |Nour        |mah306@mail.com  |84       |\n",
      "|717       |Mahmoud     |far696@mail.com  |235      |\n",
      "|722       |Eman        |nou701@mail.com  |331      |\n",
      "|898       |Unknown     |mah870@mail.com  |213      |\n",
      "|1085      |Mahmoud     |ade1054@mail.com |228      |\n",
      "+----------+------------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "students_clean = students_clean.dropna(subset=[\"birth_date\",\"family_id\"])\n",
    "students_clean = students_clean.dropDuplicates([\"student_name\", \"email\",\"family_id\"])\n",
    "\n",
    "students_clean.select(\"student_id\", \"student_name\",\"email\",\"family_id\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bbdf947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_date \n",
    "students_clean = students_clean.withColumn(\"effective_date\", current_date()) \\\n",
    "                   .withColumn(\"end_date\", lit(\"9999-12-31\")) \\\n",
    "                   .withColumn(\"current_flag\", lit(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bde80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4672b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e70ca84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n",
      "+---------+----------------+-----------------+--------+\n",
      "|course_id|course_name     |teacher_name     |semester|\n",
      "+---------+----------------+-----------------+--------+\n",
      "|1        |Mathematics     |NULL             |1       |\n",
      "|2        |Physics         |NULL             |1       |\n",
      "|3        |Chemistry       |NULL             |1       |\n",
      "|4        |English         |Dr. Mona Hassan  |1       |\n",
      "|5        |Arabic          |Dr. Heba Youssef |1       |\n",
      "|6        |Biology         |Dr. Karim Fathy  |2       |\n",
      "|7        |History         |Dr. Yara Mostafa |2       |\n",
      "|8        |Geography       |Dr. Adel Samir   |2       |\n",
      "|9        |Computer Science|Dr. Hossam Tarek |2       |\n",
      "|10       |Philosophy      |Dr. Rana Ahmed   |2       |\n",
      "|11       |French          |Dr. Sarah Ibrahim|1       |\n",
      "|12       |Religion        |Dr. Tarek Mahmoud|1       |\n",
      "|13       |Economics       |Dr. Waleed Fathy |2       |\n",
      "|14       |Sociology       |Dr. Heba Magdy   |2       |\n",
      "|15       |Statistics      |Dr. Nader Zaki   |2       |\n",
      "+---------+----------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_df = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT * FROM dbo.courses) AS t\",\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\")\n",
    "courses_df.show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b09ad1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-----------------+--------+\n",
      "|course_id|course_name     |teacher_name     |semester|\n",
      "+---------+----------------+-----------------+--------+\n",
      "|1        |Mathematics     |Unknown Teacher  |1       |\n",
      "|2        |Physics         |Unknown Teacher  |1       |\n",
      "|3        |Chemistry       |Unknown Teacher  |1       |\n",
      "|4        |English         |Dr. Mona Hassan  |1       |\n",
      "|5        |Arabic          |Dr. Heba Youssef |1       |\n",
      "|6        |Biology         |Dr. Karim Fathy  |2       |\n",
      "|7        |History         |Dr. Yara Mostafa |2       |\n",
      "|8        |Geography       |Dr. Adel Samir   |2       |\n",
      "|9        |Computer Science|Dr. Hossam Tarek |2       |\n",
      "|10       |Philosophy      |Dr. Rana Ahmed   |2       |\n",
      "|11       |French          |Dr. Sarah Ibrahim|1       |\n",
      "|12       |Religion        |Dr. Tarek Mahmoud|1       |\n",
      "|13       |Economics       |Dr. Waleed Fathy |2       |\n",
      "|14       |Sociology       |Dr. Heba Magdy   |2       |\n",
      "|15       |Statistics      |Dr. Nader Zaki   |2       |\n",
      "+---------+----------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_clean = courses_df.fillna({\n",
    "    \"teacher_name\": \"Unknown Teacher\",\n",
    "    \"semester\": \"1\"\n",
    "})\n",
    "courses_clean.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84a66414",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_student_df = students_clean.crossJoin(courses_clean) \\\n",
    "    .withColumn(\"enrollment_date\", current_date()) \\\n",
    "    .withColumn(\"completion_date\", lit(None)) \\\n",
    "    .withColumn(\"grade\", lit(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59353d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n",
      "+-------------+----------+---------+---------------+\n",
      "|enrollment_id|student_id|course_id|enrollment_date|\n",
      "+-------------+----------+---------+---------------+\n",
      "|1            |965       |1        |2024-05-27     |\n",
      "|2            |1123      |2        |2024-10-02     |\n",
      "|3            |1354      |8        |2024-03-30     |\n",
      "|4            |173       |15       |2025-02-15     |\n",
      "|5            |1287      |NULL     |2025-06-13     |\n",
      "|6            |1385      |5        |2025-09-02     |\n",
      "|7            |1153      |10       |2024-05-12     |\n",
      "|8            |184       |4        |2023-08-01     |\n",
      "|9            |NULL      |9        |2025-08-01     |\n",
      "|10           |461       |1        |2025-08-07     |\n",
      "|11           |492       |5        |2025-04-23     |\n",
      "|12           |232       |14       |2023-07-14     |\n",
      "|13           |19        |13       |2025-01-08     |\n",
      "|14           |973       |9        |2023-08-07     |\n",
      "|15           |948       |12       |2025-03-28     |\n",
      "|16           |1108      |NULL     |2024-10-28     |\n",
      "|17           |719       |7        |2023-11-22     |\n",
      "|18           |1168      |9        |2025-05-22     |\n",
      "|19           |1350      |1        |2025-06-13     |\n",
      "|20           |881       |9        |2024-11-20     |\n",
      "+-------------+----------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enroll_df = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT * FROM dbo.Enrollments) AS t\",\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\")\n",
    "enroll_df.show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa14ef88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------+---------------+\n",
      "|enrollment_id|student_id|course_id|enrollment_date|\n",
      "+-------------+----------+---------+---------------+\n",
      "|180          |79        |10       |2024-04-26     |\n",
      "|250          |83        |11       |2024-12-04     |\n",
      "|385          |1010      |4        |2025-07-18     |\n",
      "|387          |964       |14       |2023-10-25     |\n",
      "|605          |309       |9        |2024-01-15     |\n",
      "|733          |1023      |15       |2024-05-01     |\n",
      "|735          |1326      |6        |2023-06-14     |\n",
      "|862          |276       |11       |2025-09-22     |\n",
      "|1397         |1307      |4        |2023-06-28     |\n",
      "|1830         |336       |5        |2023-08-12     |\n",
      "|1974         |794       |13       |2024-09-10     |\n",
      "|2021         |426       |13       |2024-10-10     |\n",
      "|2066         |973       |10       |2023-04-26     |\n",
      "|2458         |114       |15       |2025-05-20     |\n",
      "|2517         |636       |8        |2024-12-05     |\n",
      "|2553         |1408      |6        |2024-07-05     |\n",
      "|2864         |858       |4        |2024-11-17     |\n",
      "|2904         |92        |2        |2024-01-01     |\n",
      "|934          |902       |15       |2023-11-02     |\n",
      "|1073         |48        |6        |2023-08-08     |\n",
      "+-------------+----------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enroll_clean = enroll_df.dropDuplicates([\"student_id\", \"course_id\"])\n",
    "enroll_clean = enroll_clean.dropna(subset=[\"student_id\", \"course_id\"])\n",
    "enroll_clean.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f8f651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n",
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "|family_id|father_name   |mother_name |contact_number |address   |email                    |\n",
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "|701      |Tarek         |NULL        |0109.69622e+007|Suez      |tarek.1@gmail.com        |\n",
      "|702      |Ahmed         |NULL        |0102.5946e+007 |Zagazig   |ahmed.2@gmail.com        |\n",
      "|703      |Hassan Ibrahim|Noura Samira|0109.77152e+007|Aswan     |hassan.ibrahim3@gmail.com|\n",
      "|704      |Youssef       |Abeer       |0109.01294e+006|Giza      |youssef.4@gmail.com      |\n",
      "|705      | Zaki         |Hoda Hassan |0107.29133e+007|Alexandria|.zaki5@gmail.com         |\n",
      "|706      |Ahmed         |Sara Tarek  |NULL           |Suez      |ahmed.6@gmail.com        |\n",
      "|707      | Nabil        |            |0106.338e+007  |Tanta     |.nabil7@gmail.com        |\n",
      "|708      |Omar Fathy    |Walaa       |0107.78859e+007|Minya     |omar.fathy8@gmail.com    |\n",
      "|709      |Khaled Ali    |NULL        |0105.90575e+007|Tanta     |khaled.ali9@gmail.com    |\n",
      "|710      |Mohamed       |NULL        |0105.75866e+007|Mansoura  |mohamed.10@gmail.com     |\n",
      "|711      |Mohamed Fathy |Rania       |0104.9035e+007 |Suez      |mohamed.fathy11@gmail.com|\n",
      "|712      |Youssef Said  |NULL        |0105.85111e+007|Mansoura  |youssef.said12@gmail.com |\n",
      "|713      |Youssef Samir |Noura Eman  |0105.88789e+007|Aswan     |NULL                     |\n",
      "|714      | Ibrahim      |Walaa       |0103.82646e+006|Minya     |.ibrahim14@gmail.com     |\n",
      "|715      |Tarek Adel    |Salma Tarek |0104.4458e+007 |Suez      |tarek.adel15@gmail.com   |\n",
      "|716      |              |Walaa       |0109.05227e+007|Alexandria|.16@gmail.com            |\n",
      "|717      |              | Eman       |0105.6959e+007 |Alexandria|.17@gmail.com            |\n",
      "|718      |Youssef       |Walaa Nour  |0109.38262e+007|Aswan     |NULL                     |\n",
      "|719      |              |Heba Tarek  |0105.33906e+007|Tanta     |.19@gmail.com            |\n",
      "|720      |Khaled        |NULL        |0109.59753e+007|Tanta     |khaled.20@gmail.com      |\n",
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "family_df = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT * FROM dbo.Family) AS t\",\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\")\n",
    "family_df.show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd4c0345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "|family_id|father_name   |mother_name |contact_number |address   |email                    |\n",
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "|701      |Tarek         |NULL        |0109.69622e+007|Suez      |tarek.1@gmail.com        |\n",
      "|702      |Ahmed         |NULL        |0102.5946e+007 |Zagazig   |ahmed.2@gmail.com        |\n",
      "|703      |Hassan Ibrahim|Noura Samira|0109.77152e+007|Aswan     |hassan.ibrahim3@gmail.com|\n",
      "|704      |Youssef       |Abeer       |0109.01294e+006|Giza      |youssef.4@gmail.com      |\n",
      "|705      | Zaki         |Hoda Hassan |0107.29133e+007|Alexandria|.zaki5@gmail.com         |\n",
      "|706      |Ahmed         |Sara Tarek  |NULL           |Suez      |ahmed.6@gmail.com        |\n",
      "|707      | Nabil        |NULL        |0106.338e+007  |Tanta     |.nabil7@gmail.com        |\n",
      "|708      |Omar Fathy    |Walaa       |0107.78859e+007|Minya     |omar.fathy8@gmail.com    |\n",
      "|709      |Khaled Ali    |NULL        |0105.90575e+007|Tanta     |khaled.ali9@gmail.com    |\n",
      "|710      |Mohamed       |NULL        |0105.75866e+007|Mansoura  |mohamed.10@gmail.com     |\n",
      "|711      |Mohamed Fathy |Rania       |0104.9035e+007 |Suez      |mohamed.fathy11@gmail.com|\n",
      "|712      |Youssef Said  |NULL        |0105.85111e+007|Mansoura  |youssef.said12@gmail.com |\n",
      "|713      |Youssef Samir |Noura Eman  |0105.88789e+007|Aswan     |NULL                     |\n",
      "|714      | Ibrahim      |Walaa       |0103.82646e+006|Minya     |.ibrahim14@gmail.com     |\n",
      "|715      |Tarek Adel    |Salma Tarek |0104.4458e+007 |Suez      |tarek.adel15@gmail.com   |\n",
      "|716      |NULL          |Walaa       |0109.05227e+007|Alexandria|.16@gmail.com            |\n",
      "|717      |NULL          | Eman       |0105.6959e+007 |Alexandria|.17@gmail.com            |\n",
      "|718      |Youssef       |Walaa Nour  |0109.38262e+007|Aswan     |NULL                     |\n",
      "|719      |NULL          |Heba Tarek  |0105.33906e+007|Tanta     |.19@gmail.com            |\n",
      "|720      |Khaled        |NULL        |0109.59753e+007|Tanta     |khaled.20@gmail.com      |\n",
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "family_clean_ = family_df.replace(' ', None)\n",
    "family_clean_=family_clean_.dropna(subset=[\"family_id\"])\n",
    "\n",
    "family_clean_.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52de54e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "|family_id|father_name   |mother_name |contact_number |address   |email                    |\n",
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "|701      |Tarek         |Unknown     |0109.69622e+007|Suez      |tarek.1@gmail.com        |\n",
      "|702      |Ahmed         |Unknown     |0102.5946e+007 |Zagazig   |ahmed.2@gmail.com        |\n",
      "|703      |Hassan Ibrahim|Noura Samira|0109.77152e+007|Aswan     |hassan.ibrahim3@gmail.com|\n",
      "|704      |Youssef       |Abeer       |0109.01294e+006|Giza      |youssef.4@gmail.com      |\n",
      "|705      | Zaki         |Hoda Hassan |0107.29133e+007|Alexandria|.zaki5@gmail.com         |\n",
      "|706      |Ahmed         |Sara Tarek  |01000000000    |Suez      |ahmed.6@gmail.com        |\n",
      "|707      | Nabil        |Unknown     |0106.338e+007  |Tanta     |.nabil7@gmail.com        |\n",
      "|708      |Omar Fathy    |Walaa       |0107.78859e+007|Minya     |omar.fathy8@gmail.com    |\n",
      "|709      |Khaled Ali    |Unknown     |0105.90575e+007|Tanta     |khaled.ali9@gmail.com    |\n",
      "|710      |Mohamed       |Unknown     |0105.75866e+007|Mansoura  |mohamed.10@gmail.com     |\n",
      "|711      |Mohamed Fathy |Rania       |0104.9035e+007 |Suez      |mohamed.fathy11@gmail.com|\n",
      "|712      |Youssef Said  |Unknown     |0105.85111e+007|Mansoura  |youssef.said12@gmail.com |\n",
      "|713      |Youssef Samir |Noura Eman  |0105.88789e+007|Aswan     |unknown@gmail.com        |\n",
      "|714      | Ibrahim      |Walaa       |0103.82646e+006|Minya     |.ibrahim14@gmail.com     |\n",
      "|715      |Tarek Adel    |Salma Tarek |0104.4458e+007 |Suez      |tarek.adel15@gmail.com   |\n",
      "|716      |Unknown       |Walaa       |0109.05227e+007|Alexandria|.16@gmail.com            |\n",
      "|717      |Unknown       | Eman       |0105.6959e+007 |Alexandria|.17@gmail.com            |\n",
      "|718      |Youssef       |Walaa Nour  |0109.38262e+007|Aswan     |unknown@gmail.com        |\n",
      "|719      |Unknown       |Heba Tarek  |0105.33906e+007|Tanta     |.19@gmail.com            |\n",
      "|720      |Khaled        |Unknown     |0109.59753e+007|Tanta     |khaled.20@gmail.com      |\n",
      "+---------+--------------+------------+---------------+----------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "family_clean_ = family_clean_.fillna({\n",
    "    \"address\": \"Unknown\",\n",
    "    \"email\": \"unknown@gmail.com\",\n",
    "    \"contact_number\": \"01000000000\",\n",
    "    \"father_name\" : \"Unknown\",\n",
    "    \"mother_name\" : \"Unknown\",\n",
    "})\n",
    "family_clean_.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250375af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "340a42e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+------------+---------------+----------+--------------------+\n",
      "|family_id|   father_name| mother_name| contact_number|   address|               email|\n",
      "+---------+--------------+------------+---------------+----------+--------------------+\n",
      "|      701|        Tarek |     Unknown|0109.69622e+007|      Suez|   tarek.1@gmail.com|\n",
      "|      702|        Ahmed |     Unknown| 0102.5946e+007|   Zagazig|   ahmed.2@gmail.com|\n",
      "|      703|Hassan Ibrahim|Noura Samira|0109.77152e+007|     Aswan|hassan.ibrahim3@g...|\n",
      "|      704|      Youssef |      Abeer |0109.01294e+006|      Giza| youssef.4@gmail.com|\n",
      "|      705|          Zaki| Hoda Hassan|0107.29133e+007|Alexandria|    .zaki5@gmail.com|\n",
      "|      706|        Ahmed |  Sara Tarek|    01000000000|      Suez|   ahmed.6@gmail.com|\n",
      "|      707|         Nabil|     Unknown|  0106.338e+007|     Tanta|   .nabil7@gmail.com|\n",
      "|      708|    Omar Fathy|      Walaa |0107.78859e+007|     Minya|omar.fathy8@gmail...|\n",
      "|      709|    Khaled Ali|     Unknown|0105.90575e+007|     Tanta|khaled.ali9@gmail...|\n",
      "|      710|      Mohamed |     Unknown|0105.75866e+007|  Mansoura|mohamed.10@gmail.com|\n",
      "|      711| Mohamed Fathy|      Rania | 0104.9035e+007|      Suez|mohamed.fathy11@g...|\n",
      "|      712|  Youssef Said|     Unknown|0105.85111e+007|  Mansoura|youssef.said12@gm...|\n",
      "|      713| Youssef Samir|  Noura Eman|0105.88789e+007|     Aswan|   unknown@gmail.com|\n",
      "|      714|       Ibrahim|      Walaa |0103.82646e+006|     Minya|.ibrahim14@gmail.com|\n",
      "|      715|    Tarek Adel| Salma Tarek| 0104.4458e+007|      Suez|tarek.adel15@gmai...|\n",
      "|      716|       Unknown|      Walaa |0109.05227e+007|Alexandria|       .16@gmail.com|\n",
      "|      717|       Unknown|        Eman| 0105.6959e+007|Alexandria|       .17@gmail.com|\n",
      "|      718|      Youssef |  Walaa Nour|0109.38262e+007|     Aswan|   unknown@gmail.com|\n",
      "|      719|       Unknown|  Heba Tarek|0105.33906e+007|     Tanta|       .19@gmail.com|\n",
      "|      720|       Khaled |     Unknown|0109.59753e+007|     Tanta| khaled.20@gmail.com|\n",
      "+---------+--------------+------------+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "family_clean=family_clean_\n",
    "family_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b864c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "family_clean = family_clean.withColumn(\"family_email\", lit(\"unknown@gmail.com\")) \\\n",
    "                           .withColumn(\"family_address\", lit(\"Unknown\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0233594",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_clean = family_clean.drop(\"family_email\", \"family_address\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a92e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "family_clean = family_clean.fillna({\n",
    "    \"father_name\": \"Unknown\",\n",
    "    \"mother_name\": \"Unknown\",\n",
    "    \"contact_number\": \"0000000000\",\n",
    "    \"address\": \"Unknown\",\n",
    "    \"email\": \"N/A\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edef1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "family_clean = family_clean.withColumn(\"family_id\", \n",
    "                                                 lit(None).cast(\"int\")) \\\n",
    "                                      .withColumn(\"family_id\", \n",
    "                                                  monotonically_increasing_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c430f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_clean = family_clean.drop(\"family_email\", \"family_address\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de64bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "family_clean = family_clean.withColumn(\"family_id\", col(\"family_id\").cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3b5ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_clean = family_clean.fillna({\n",
    "    \"father_name\": \"Unknown\",\n",
    "    \"mother_name\": \"Unknown\",\n",
    "    \"email\": \"N/A\",\n",
    "    \"contact_number\": \"N/A\",\n",
    "    \"address\": \"N/A\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac31213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n",
      "+----------+------------+-----------+----------+----------+\n",
      "|teacher_id|teacher_name|phone      |hire_date |department|\n",
      "+----------+------------+-----------+----------+----------+\n",
      "|T001      |Ahmed Ali   |01012345678|2020-09-01|Math      |\n",
      "|T002      |Sara Mohamed|01087654321|2019-01-15|Physics   |\n",
      "|T003      |Omar Hassan |01011223344|2021-06-20|Chemistry |\n",
      "+----------+------------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_clean = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT * FROM dbo.DimTeacher) AS t\",\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\")\n",
    "teacher_clean.show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b03d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "130ee1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n",
      "+-------+----------+---------+-----+----------+\n",
      "|exam_id|student_id|course_id|score|exam_date |\n",
      "+-------+----------+---------+-----+----------+\n",
      "|1      |1453      |15       |12.0 |2022-12-11|\n",
      "|2      |878       |10       |77.65|2025-08-25|\n",
      "|3      |1111      |2        |64.6 |2024-05-26|\n",
      "|4      |866       |5        |43.07|2022-10-12|\n",
      "|5      |1272      |NULL     |81.59|2025-07-03|\n",
      "|6      |1171      |1        |37.35|2022-12-06|\n",
      "|7      |976       |6        |72.99|2024-03-09|\n",
      "|8      |NULL      |10       |56.66|2025-05-27|\n",
      "|9      |1432      |11       |76.35|2022-08-20|\n",
      "|10     |960       |1        |88.61|2024-11-05|\n",
      "|11     |1472      |13       |51.17|2024-04-06|\n",
      "|12     |965       |9        |64.23|2025-06-24|\n",
      "|13     |298       |10       |67.7 |2022-09-23|\n",
      "|14     |122       |NULL     |NULL |2022-07-27|\n",
      "|15     |NULL      |5        |20.62|2023-09-16|\n",
      "|16     |867       |2        |NULL |2023-04-22|\n",
      "|17     |527       |3        |NULL |2023-05-19|\n",
      "|18     |1046      |8        |2.48 |2023-02-24|\n",
      "|19     |702       |3        |73.71|2023-09-02|\n",
      "|20     |146       |7        |46.04|2023-04-26|\n",
      "+-------+----------+---------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_df = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT * FROM dbo.Exams) AS t\",\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\")\n",
    "exams_df.show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe895a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+-----+----------+\n",
      "|exam_id|student_id|course_id|score|exam_date |\n",
      "+-------+----------+---------+-----+----------+\n",
      "|1      |1453      |15       |12.0 |2022-12-11|\n",
      "|2      |878       |10       |77.65|2025-08-25|\n",
      "|3      |1111      |2        |64.6 |2024-05-26|\n",
      "|4      |866       |5        |43.07|2022-10-12|\n",
      "|6      |1171      |1        |37.35|2022-12-06|\n",
      "|7      |976       |6        |72.99|2024-03-09|\n",
      "|8      |NULL      |10       |56.66|2025-05-27|\n",
      "|9      |1432      |11       |76.35|2022-08-20|\n",
      "|10     |960       |1        |88.61|2024-11-05|\n",
      "|11     |1472      |13       |51.17|2024-04-06|\n",
      "|12     |965       |9        |64.23|2025-06-24|\n",
      "|13     |298       |10       |67.7 |2022-09-23|\n",
      "|15     |NULL      |5        |20.62|2023-09-16|\n",
      "|16     |867       |2        |NULL |2023-04-22|\n",
      "|17     |527       |3        |NULL |2023-05-19|\n",
      "|18     |1046      |8        |2.48 |2023-02-24|\n",
      "|19     |702       |3        |73.71|2023-09-02|\n",
      "|20     |146       |7        |46.04|2023-04-26|\n",
      "|21     |843       |5        |36.87|2023-06-24|\n",
      "|22     |1341      |7        |10.99|2022-09-30|\n",
      "+-------+----------+---------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_clean=exams_df.dropna(subset=[\"student_id\"])\n",
    "exams_clean=exams_df.dropna(subset=[\"course_id\"])\n",
    "\n",
    "exams_clean.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0baef405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as Fn \n",
    "avg_score = exams_df.select(Fn.avg(Fn.col(\"score\"))).collect()[0][0]\n",
    "exams_clean = exams_clean.fillna({\"score\": avg_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e61646ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------------------+----------+\n",
      "|exam_id|student_id|course_id|score             |exam_date |\n",
      "+-------+----------+---------+------------------+----------+\n",
      "|1      |1453      |15       |12.0              |2022-12-11|\n",
      "|2      |878       |10       |77.65             |2025-08-25|\n",
      "|3      |1111      |2        |64.6              |2024-05-26|\n",
      "|4      |866       |5        |43.07             |2022-10-12|\n",
      "|6      |1171      |1        |37.35             |2022-12-06|\n",
      "|7      |976       |6        |72.99             |2024-03-09|\n",
      "|8      |NULL      |10       |56.66             |2025-05-27|\n",
      "|9      |1432      |11       |76.35             |2022-08-20|\n",
      "|10     |960       |1        |88.61             |2024-11-05|\n",
      "|11     |1472      |13       |51.17             |2024-04-06|\n",
      "|12     |965       |9        |64.23             |2025-06-24|\n",
      "|13     |298       |10       |67.7              |2022-09-23|\n",
      "|15     |NULL      |5        |20.62             |2023-09-16|\n",
      "|16     |867       |2        |49.984948485794554|2023-04-22|\n",
      "|17     |527       |3        |49.984948485794554|2023-05-19|\n",
      "|18     |1046      |8        |2.48              |2023-02-24|\n",
      "|19     |702       |3        |73.71             |2023-09-02|\n",
      "|20     |146       |7        |46.04             |2023-04-26|\n",
      "|21     |843       |5        |36.87             |2023-06-24|\n",
      "|22     |1341      |7        |10.99             |2022-09-30|\n",
      "+-------+----------+---------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_clean.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd3eb4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ø¹Ù…Ù„ Ø§Ù„Ø¹Ù…ÙˆØ¯ result_flag\n",
    "exams_clean = exams_clean.withColumn(\n",
    "    \"result_flag\",\n",
    "    when(col(\"score\") >= 50, True).otherwise(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d5359d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\n",
      "+-------------+----------+---------+---------------+-------+\n",
      "|attendance_id|student_id|course_id|attendance_date|status |\n",
      "+-------------+----------+---------+---------------+-------+\n",
      "|1            |174       |10       |2024-10-15     |Present|\n",
      "|2            |1134      |3        |2024-01-02     |Absent |\n",
      "|3            |NULL      |8        |2024-07-19     |Present|\n",
      "|4            |951       |1        |2025-10-22     |Present|\n",
      "|5            |715       |12       |2025-02-13     |Present|\n",
      "|6            |329       |13       |2025-05-01     |Present|\n",
      "|7            |1226      |3        |2025-01-06     |Present|\n",
      "|8            |36        |NULL     |2025-08-01     |Present|\n",
      "|9            |777       |5        |2024-03-24     |Absent |\n",
      "|10           |775       |4        |2025-01-08     |Absent |\n",
      "|11           |985       |8        |2024-04-16     |Absent |\n",
      "|12           |1254      |14       |2024-05-08     |Absent |\n",
      "|13           |1292      |12       |2024-12-11     |Present|\n",
      "|14           |907       |1        |2024-06-23     |Present|\n",
      "|15           |NULL      |4        |2024-08-16     |Present|\n",
      "|16           |309       |3        |2025-02-28     |Present|\n",
      "|17           |941       |6        |2025-07-20     |Absent |\n",
      "|18           |404       |4        |2024-04-07     |NULL   |\n",
      "|19           |276       |2        |2023-12-30     |Absent |\n",
      "|20           |661       |7        |2025-01-26     |NULL   |\n",
      "+-------------+----------+---------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attendance_df = spark.read.jdbc(\n",
    "    url=sqlserver_url,\n",
    "    table=\"(SELECT * FROM dbo.Attendance) AS t\",\n",
    "    properties=sqlserver_props\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©:\")\n",
    "attendance_df.show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f765fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------+---------------+-------+\n",
      "|attendance_id|student_id|course_id|attendance_date|status |\n",
      "+-------------+----------+---------+---------------+-------+\n",
      "|1            |174       |10       |2024-10-15     |Present|\n",
      "|2            |1134      |3        |2024-01-02     |Absent |\n",
      "|3            |NULL      |8        |2024-07-19     |Present|\n",
      "|4            |951       |1        |2025-10-22     |Present|\n",
      "|5            |715       |12       |2025-02-13     |Present|\n",
      "|6            |329       |13       |2025-05-01     |Present|\n",
      "|7            |1226      |3        |2025-01-06     |Present|\n",
      "|9            |777       |5        |2024-03-24     |Absent |\n",
      "|10           |775       |4        |2025-01-08     |Absent |\n",
      "|11           |985       |8        |2024-04-16     |Absent |\n",
      "|12           |1254      |14       |2024-05-08     |Absent |\n",
      "|13           |1292      |12       |2024-12-11     |Present|\n",
      "|14           |907       |1        |2024-06-23     |Present|\n",
      "|15           |NULL      |4        |2024-08-16     |Present|\n",
      "|16           |309       |3        |2025-02-28     |Present|\n",
      "|17           |941       |6        |2025-07-20     |Absent |\n",
      "|18           |404       |4        |2024-04-07     |unknown|\n",
      "|19           |276       |2        |2023-12-30     |Absent |\n",
      "|20           |661       |7        |2025-01-26     |unknown|\n",
      "|22           |450       |8        |2025-07-27     |Present|\n",
      "+-------------+----------+---------+---------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attendance_clean=attendance_df.dropna(subset=[\"student_id\"])\n",
    "attendance_clean=attendance_df.dropna(subset=[\"course_id\"])\n",
    "attendance_clean = attendance_clean.fillna({\"status\": \"unknown\"})\n",
    "\n",
    "attendance_clean.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28063fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_clean.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"dimstsudents\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table Dimstudents uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_clean.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"dimfamily\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table DimFamily uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ef78072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table DimFamily uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "    \n",
    " family_clean.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"dimfamily\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table DimFamily uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4b6adeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table DimTeacher uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    " DimTeacher.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"DimTeacher\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table DimTeacher uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893da87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+----------+----------+\n",
      "|teacher_id|teacher_name|      phone| hire_date|department|\n",
      "+----------+------------+-----------+----------+----------+\n",
      "|      T001|   Ahmed Ali|01012345678|2020-09-01|      Math|\n",
      "|      T002|Sara Mohamed|01087654321|2019-01-15|   Physics|\n",
      "|      T003| Omar Hassan|01011223344|2021-06-20| Chemistry|\n",
      "+----------+------------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e5187256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# Ø§ÙØªØ±Ø¶ Ø¹Ù†Ø¯Ùƒ DataFrame courses_df Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„ÙŠ Ø¨Ø¹ØªØªÙ‡Ø§Ù„ÙŠ\n",
    "# Ø§ÙØªØ±Ø¶ Ø¹Ù†Ø¯Ùƒ DataFrame dim_teacher_df ÙÙŠÙ‡ teacher_id Ùˆ teacher_name\n",
    "\n",
    "# Ø£ÙˆÙ„ Ø­Ø§Ø¬Ø© Ù†Ø¹Ù…Ù„ join Ø¹Ø´Ø§Ù† Ù†Ø­ØµÙ„ Ø¹Ù„Ù‰ teacher_id\n",
    "courses_df_ready = courses_clean.join(\n",
    "    DimTeacher.select(\"teacher_id\", \"teacher_name\"),\n",
    "    on=\"teacher_name\",\n",
    "    how=\"left\"\n",
    ").withColumn(\"effective_date\", lit(\"2025-11-05\")) \\\n",
    " .select(\n",
    "     col(\"course_id\").cast(\"string\"),\n",
    "     col(\"course_name\").cast(\"string\"),\n",
    "     col(\"semester\").cast(\"string\"),\n",
    "     col(\"teacher_id\").cast(\"string\"),\n",
    "     col(\"effective_date\").cast(\"date\")\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "886cf84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table courses_df_ready uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    " courses_df_ready.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"courses_df_ready\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table courses_df_ready uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "790de086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, explode, sequence\n",
    "from pyspark.sql.types import DateType\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ§Ø±ÙŠØ® Ø¨ÙŠÙ† 2025-01-01 Ùˆ 2025-12-31\n",
    "start = datetime.date(2025, 1, 1)\n",
    "end = datetime.date(2025, 12, 31)\n",
    "\n",
    "date_df = spark.range(0, (end - start).days + 1) \\\n",
    "    .withColumn(\"full_date\", (lit(start) + col(\"id\").cast(\"interval day\"))) \\\n",
    "    .withColumn(\"date_id\", col(\"full_date\")) \\\n",
    "    .withColumn(\"day\", col(\"full_date\").cast(\"date\").substr(9, 2).cast(\"int\")) \\\n",
    "    .withColumn(\"month\", col(\"full_date\").cast(\"date\").substr(6, 2).cast(\"int\")) \\\n",
    "    .withColumn(\"year\", col(\"full_date\").cast(\"date\").substr(1, 4).cast(\"int\")) \\\n",
    "    .drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bbad1f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table enroll_clean uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Ø§ÙØªØ±Ø¶ Ø¹Ù†Ø¯Ùƒ DataFrame Ø§Ø³Ù…Ù‡ enrollments_df ÙÙŠÙ‡ student_id Ùˆ course_id\n",
    "# Ù„Ùˆ Ù…Ø´ Ø¹Ù†Ø¯Ùƒ ØªØ§Ø±ÙŠØ®ØŒ Ù…Ù…ÙƒÙ† ØªØ­Ø· Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø­Ø§Ù„ÙŠ Ù…Ø¤Ù‚ØªØ§Ù‹\n",
    "enroll_clean = enroll_clean.withColumn(\"date_id\", lit(\"2025-11-05\")) \\\n",
    "                                     .withColumn(\"enrollment_count\", lit(1))\n",
    "\n",
    "# Ø±ÙØ¹ Ø¹Ù„Ù‰ PostgreSQL\n",
    "enroll_clean.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"fact_enroll\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table enroll_clean uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c2f08f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table attendance_clean uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Ø§ÙØªØ±Ø¶ Ø¹Ù†Ø¯Ùƒ DataFrame Ø§Ø³Ù…Ù‡ attendance_df ÙÙŠÙ‡ student_id Ùˆ course_id\n",
    "# Ù†Ø¶ÙŠÙ ØªØ§Ø±ÙŠØ® Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù„Ø­Ø¶ÙˆØ± ÙˆØ­Ø§Ù„Ø© Ø§Ù„Ø­Ø¶ÙˆØ±\n",
    "attendance_clean = attendance_clean.withColumn(\"date_id\", lit(\"2025-11-05\")) \\\n",
    "                                   .withColumn(\"status\", lit(True))  # True ÙŠØ¹Ù†ÙŠ Ø­Ø¶Ø±\n",
    "\n",
    "# Ø±ÙØ¹ Ø¹Ù„Ù‰ PostgreSQL\n",
    "attendance_clean.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"FactAttendance\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table attendance_clean uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c4993d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table DimDate uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, explode, sequence\n",
    "from pyspark.sql.types import DateType\n",
    "import datetime\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ§Ø±ÙŠØ® Ø¨ÙŠÙ† 2025-01-01 Ùˆ 2025-12-31\n",
    "start = datetime.date(2025, 1, 1)\n",
    "end = datetime.date(2025, 12, 31)\n",
    "\n",
    "date_df = spark.range(0, (end - start).days + 1) \\\n",
    "    .withColumn(\"full_date\", (lit(start) + col(\"id\").cast(\"interval day\"))) \\\n",
    "    .withColumn(\"date_id\", col(\"full_date\")) \\\n",
    "    .withColumn(\"day\", col(\"full_date\").cast(\"date\").substr(9, 2).cast(\"int\")) \\\n",
    "    .withColumn(\"month\", col(\"full_date\").cast(\"date\").substr(6, 2).cast(\"int\")) \\\n",
    "    .withColumn(\"year\", col(\"full_date\").cast(\"date\").substr(1, 4).cast(\"int\")) \\\n",
    "    .drop(\"id\")\n",
    "\n",
    "# Ø±ÙØ¹ Ø¹Ù„Ù‰ PostgreSQL\n",
    "date_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"DimDate\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table DimDate uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6b0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------------------+----------+\n",
      "|exam_id|student_id|course_id|             score| exam_date|\n",
      "+-------+----------+---------+------------------+----------+\n",
      "|      1|      1453|       15|              12.0|2022-12-11|\n",
      "|      2|       878|       10|             77.65|2025-08-25|\n",
      "|      3|      1111|        2|              64.6|2024-05-26|\n",
      "|      4|       866|        5|             43.07|2022-10-12|\n",
      "|      6|      1171|        1|             37.35|2022-12-06|\n",
      "|      7|       976|        6|             72.99|2024-03-09|\n",
      "|      8|      NULL|       10|             56.66|2025-05-27|\n",
      "|      9|      1432|       11|             76.35|2022-08-20|\n",
      "|     10|       960|        1|             88.61|2024-11-05|\n",
      "|     11|      1472|       13|             51.17|2024-04-06|\n",
      "|     12|       965|        9|             64.23|2025-06-24|\n",
      "|     13|       298|       10|              67.7|2022-09-23|\n",
      "|     15|      NULL|        5|             20.62|2023-09-16|\n",
      "|     16|       867|        2|49.984948485794554|2023-04-22|\n",
      "|     17|       527|        3|49.984948485794554|2023-05-19|\n",
      "|     18|      1046|        8|              2.48|2023-02-24|\n",
      "|     19|       702|        3|             73.71|2023-09-02|\n",
      "|     20|       146|        7|             46.04|2023-04-26|\n",
      "|     21|       843|        5|             36.87|2023-06-24|\n",
      "|     22|      1341|        7|             10.99|2022-09-30|\n",
      "+-------+----------+---------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exams_clean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "193c0ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table exams_clean uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "  exams_clean.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"fact_exam\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "\n",
    "print(\"âœ… Table exams_clean uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9805133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"course_student_id\" in course_student_df.columns:\n",
    "    course_student_df = course_student_df.drop(\"course_student_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8c1daf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "course_student_df = course_student_df.withColumn(\"grade\", course_student_df[\"grade\"].cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "77ddc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, DateType\n",
    "\n",
    "course_student_df = course_student_df.withColumn(\"grade\", course_student_df[\"grade\"].cast(StringType()))\n",
    "course_student_df = course_student_df.withColumn(\"enrollment_date\", course_student_df[\"enrollment_date\"].cast(DateType()))\n",
    "course_student_df = course_student_df.withColumn(\"completion_date\", course_student_df[\"completion_date\"].cast(DateType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2e75ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù„Ùˆ ÙÙŠÙ‡ Ø¹Ù…ÙˆØ¯ dummy Ø£Ùˆ placeholder\n",
    "for col_name, dtype in course_student_df.dtypes:\n",
    "    if dtype == \"null\":\n",
    "        course_student_df = course_student_df.drop(col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f93d18ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_student_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"DimCourseStudent\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f70c70a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o177.save.\n: org.postgresql.util.PSQLException: ERROR: cannot drop table dimfamily because other objects depend on it\n  Detail: constraint fk_family on table dimstsudents depends on table dimfamily\n  Hint: Use DROP ... CASCADE to drop the dependent objects too.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:371)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:502)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:419)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:341)\n\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:326)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:302)\n\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:275)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1094)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.dropTable(JdbcUtils.scala:81)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:63)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mfamily_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://postgres_general:5432/dwh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdimfamily\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Table DimFamily uploaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o177.save.\n: org.postgresql.util.PSQLException: ERROR: cannot drop table dimfamily because other objects depend on it\n  Detail: constraint fk_family on table dimstsudents depends on table dimfamily\n  Hint: Use DROP ... CASCADE to drop the dependent objects too.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:371)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:502)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:419)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:341)\n\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:326)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:302)\n\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:275)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1094)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.dropTable(JdbcUtils.scala:81)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:63)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "family_clean.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres_general:5432/dwh\") \\\n",
    "    .option(\"dbtable\", \"dimfamily\") \\\n",
    "    .options(user=\"admin\", password=\"admin\", driver=\"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table DimFamily uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f4d27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_clean.write\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", postgres_url)\\\n",
    "    .option(\"dbtable\", \"dimfamily\")\\\n",
    "    .options(**postgres_props)\\\n",
    "    .mode(\"append\")\\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d87d187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Table Dimstudents uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "students_clean.write \\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", postgres_url)\\\n",
    "    .option(\"dbtable\", \"dimstsudents\")\\\n",
    "    .options(**postgres_props)\\\n",
    "    .mode(\"append\")\\\n",
    "    .save()\n",
    "\n",
    "print(\"âœ… Table Dimstudents uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68ee2c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- full_date: date (nullable = false)\n",
      " |-- date_id: date (nullable = false)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb7301",
   "metadata": {},
   "source": [
    "#break\n",
    "##########################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e9b24bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+--------+-----------+-----------+\n",
      "|schemaname| tablename|tableowner|tablespace|hasindexes|hasrules|hastriggers|rowsecurity|\n",
      "+----------+----------+----------+----------+----------+--------+-----------+-----------+\n",
      "|    public| dimfamily|     admin|      NULL|      true|   false|       true|      false|\n",
      "|    public|dimstudent|     admin|      NULL|      true|   false|       true|      false|\n",
      "|    public|dimteacher|     admin|      NULL|      true|   false|       true|      false|\n",
      "|    public| dimcourse|     admin|      NULL|      true|   false|       true|      false|\n",
      "|    public|  factexam|     admin|      NULL|     false|   false|       true|      false|\n",
      "+----------+----------+----------+----------+----------+--------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "jdbc_url_postgres = \"jdbc:postgresql://postgres_general:5432/dwh1\"\n",
    "jdbc_props_postgres = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# test connection\n",
    "spark.read.jdbc(url=jdbc_url_postgres, table=\"pg_tables\", properties=jdbc_props_postgres).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8bc1a73d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o469.jdbc.\n: org.postgresql.util.PSQLException: ERROR: cannot drop table dimstudent because other objects depend on it\n  Detail: constraint factexam_student_id_fkey on table factexam depends on table dimstudent\nconstraint factattendance_student_id_fkey on table factattendance depends on table dimstudent\nconstraint dimcoursestudent_student_id_fkey on table dimcoursestudent depends on table dimstudent\n  Hint: Use DROP ... CASCADE to drop the dependent objects too.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:371)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:502)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:419)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:341)\n\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:326)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:302)\n\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:275)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1094)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.dropTable(JdbcUtils.scala:81)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:63)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m jdbc_props_postgres \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.postgresql.Driver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1ï¸âƒ£ Load Dimensions\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mstudents_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_url_postgres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdimstudent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjdbc_props_postgres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m teacher_clean\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39mjdbc_url_postgres, table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimteacher\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties\u001b[38;5;241m=\u001b[39mjdbc_props_postgres)\n\u001b[1;32m     14\u001b[0m course_student_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mjdbc(url\u001b[38;5;241m=\u001b[39mjdbc_url_postgres, table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimcourse\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m, properties\u001b[38;5;241m=\u001b[39mjdbc_props_postgres)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o469.jdbc.\n: org.postgresql.util.PSQLException: ERROR: cannot drop table dimstudent because other objects depend on it\n  Detail: constraint factexam_student_id_fkey on table factexam depends on table dimstudent\nconstraint factattendance_student_id_fkey on table factattendance depends on table dimstudent\nconstraint dimcoursestudent_student_id_fkey on table dimcoursestudent depends on table dimstudent\n  Hint: Use DROP ... CASCADE to drop the dependent objects too.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:371)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:502)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:419)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:341)\n\tat org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:326)\n\tat org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:302)\n\tat org.postgresql.jdbc.PgStatement.executeUpdate(PgStatement.java:275)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:1094)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.dropTable(JdbcUtils.scala:81)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:63)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "jdbc_url_postgres = \"jdbc:postgresql://postgres_general:5432/dwh1\"\n",
    "\n",
    "jdbc_props_postgres = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# 1ï¸âƒ£ Load Dimensions\n",
    "\n",
    "\n",
    "students_clean.write.jdbc(url=jdbc_url_postgres, table=\"dimstudent\", mode=\"overwrite\", properties=jdbc_props_postgres)\n",
    "teacher_clean.write.jdbc(url=jdbc_url_postgres, table=\"dimteacher\", mode=\"overwrite\", properties=jdbc_props_postgres)\n",
    "course_student_df.write.jdbc(url=jdbc_url_postgres, table=\"dimcourse\", mode=\"overwrite\", properties=jdbc_props_postgres)\n",
    "date_df.write.jdbc(url=jdbc_url_postgres, table=\"dimdate\", mode=\"overwrite\", properties=jdbc_props_postgres)\n",
    "\n",
    "exams_clean.write.jdbc(url=jdbc_url_postgres, table=\"factexam\", mode=\"overwrite\", properties=jdbc_props_postgres)\n",
    "attendance_clean.write.jdbc(url=jdbc_url_postgres, table=\"factattendance\", mode=\"overwrite\", properties=jdbc_props_postgres)\n",
    "enroll_clean.write.jdbc(url=jdbc_url_postgres, table=\"factenrollment\", mode=\"overwrite\", properties=jdbc_props_postgres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c87e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'family_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m family_clean \u001b[38;5;241m=\u001b[39m \u001b[43mfamily_clean\u001b[49m\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m students_clean \u001b[38;5;241m=\u001b[39m students_clean\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudent_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m teacher_clean \u001b[38;5;241m=\u001b[39m teacher_clean\u001b[38;5;241m.\u001b[39mdropDuplicates([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mteacher_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'family_clean' is not defined"
     ]
    }
   ],
   "source": [
    "family_clean = family_clean.dropDuplicates([\"email\"])\n",
    "students_clean = students_clean.dropDuplicates([\"student_id\"])\n",
    "teacher_clean = teacher_clean.dropDuplicates([\"teacher_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad637b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e562f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 37922)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "family_clean = family_clean.withColumn(\"family_id\", col(\"family_id\").cast(\"int\"))\n",
    "teacher_clean = teacher_clean.withColumn(\"teacher_id\", col(\"teacher_id\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c87bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "family_clean.write.jdbc(url=jdbc_url_postgres, table=\"dimfamily\", mode=\"append\", properties=jdbc_props_postgres)\n",
    "students_clean.write.jdbc(url=jdbc_url_postgres, table=\"dimstudent\", mode=\"append\", properties=jdbc_props_postgres)\n",
    "teacher_clean.write.jdbc(url=jdbc_url_postgres, table=\"dimteacher\", mode=\"append\", properties=jdbc_props_postgres)\n",
    "enroll_Clean.write.jdbc(url=jdbc_url_postgres, table=\"factenrollment\", mode=\"append\", properties=jdbc_props_postgres)\n",
    "course_student_df.write.jdbc(url=jdbc_url_postgres, table=\"factcoursestudent\", mode=\"append\", properties=jdbc_props_postgres)\n",
    "exams_clean.write.jdbc(url=jdbc_url_postgres, table=\"factexam\", mode=\"append\", properties=jdbc_props_postgres)\n",
    "attendance_clean.write.jdbc(url=jdbc_url_postgres, table=\"factattendance\", mode=\"append\", properties=jdbc_props_postgres)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
